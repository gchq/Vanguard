@article{Bike,
  author  = {Hadi Fanaee-T and Joao Gama},
  title   = {Event labeling combining ensemble detectors and background knowledge},
  year    = {2014},
  month   = {6},
  journal = {Progress in Artificial Intelligence},
  volume  = {2},
  pages   = {113--127},
  url     = {https://doi.org/10.1007/s13748-013-0040-3},
}

@inproceedings{Bonilla07,
  author    = {Bonilla, Edwin V and Chai, Kian and Williams, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {J. Platt and D. Koller and Y. Singer and S. Roweis},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Multi-task Gaussian Process Prediction},
  url       = {https://proceedings.neurips.cc/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
  volume    = {20},
  year      = {2008},
}

@misc{Burt20,
  title         = {Variational Orthogonal Features},
  author        = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
  year          = {2020},
  eprint        = {2006.13170},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
}

@article{Cao14,
  author     = {Yanshuai Cao and David J. Fleet},
  title      = {Generalized Product of Experts for Automatic and Principled Fusion of Gaussian Process Predictions},
  journal    = {CoRR},
  volume     = {abs/1410.7827},
  year       = {2014},
  url        = {http://arxiv.org/abs/1410.7827},
  eprinttype = {arXiv},
  eprint     = {1410.7827},
  timestamp  = {Mon, 13 Aug 2018 16:47:02 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/CaoF14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
}

@inproceedings{Cheng17,
  author    = {Cheng, Ching-An and Boots, Byron},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  title     = {Variational Inference for Gaussian Process Models with Linear Complexity},
  url       = {https://proceedings.neurips.cc/paper/2017/file/f8da71e562ff44a2bc7edf3578c593da-Paper.pdf},
  volume    = {30},
  year      = {2017},
}

@inproceedings{Deisenroth15,
  title     = {Distributed Gaussian Processes},
  author    = {Deisenroth, Marc and Ng, Jun Wei},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages     = {1481--1490},
  year      = {2015},
  editor    = {Bach, Francis and Blei, David},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v37/deisenroth15.pdf},
  url       = {https://proceedings.mlr.press/v37/deisenroth15.html},
  abstract  = {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian
               Committee Machine (rBCM), a practical and scalable product-of-experts model for
               large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations,
               the rBCM is conceptually simple and does not rely on inducing or variational parameters.
               The key idea is to recursively distribute computations to independent computational units
               and, subsequently, recombine them to form an overall result. Efficient closed-form inference
               allows for straightforward parallelisation and distributed computations with a small memory
               footprint. The rBCM is independent of the computational graph and can be used on heterogeneous
               computing infrastructures, ranging from laptops to clusters. With sufficient computing
               resources our distributed GP model can handle arbitrarily large data sets.},
}

@inproceedings{Hensman15,
  title     = {Scalable Variational Gaussian Process Classification},
  author    = {Hensman, James and Matthews, Alexander and Ghahramani, Zoubin},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages     = {351--360},
  year      = {2015},
  editor    = {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume    = {38},
  series    = {Proceedings of Machine Learning Research},
  address   = {San Diego, California, USA},
  month     = {09--12 May},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v38/hensman15.pdf},
  url       = {https://proceedings.mlr.press/v38/hensman15.html},
  abstract  = {Gaussian process classification is a popular method with a number of appealing properties. We
               show how to scale the model within a variational inducing point framework, out-performing the
               state of the art on benchmark datasets. Importantly, the variational formulation an be exploited
               to allow classification in problems with millions of data points, as we demonstrate in experiments.},
}

@article{Kuss05,
  author     = {Kuss, Malte and Rasmussen, Carl Edward},
  title      = {Assessing Approximate Inference for Binary Gaussian Process Classification},
  year       = {2005},
  issue_date = {12/1/2005},
  publisher  = {JMLR.org},
  volume     = {6},
  issn       = {1532-4435},
  abstract   = {Gaussian process priors can be used to define flexible, probabilistic classification
                models. Unfortunately exact Bayesian inference is analytically intractable and various
                approximation techniques have been proposed. In this work we review and compare Laplace's
                method and Expectation Propagation for approximate Bayesian inference in the binary
                Gaussian process classification model. We present a comprehensive comparison of the
                approximations, their predictive performance and marginal likelihood estimates to
                results obtained by MCMC sampling. We explain theoretically and corroborate empirically
                the advantages of Expectation Propagation compared to Laplace's method.},
  journal    = {J. Mach. Learn. Res.},
  month      = {12},
  pages      = {1679-1704},
  numpages   = {26},
  url        = {https://dl.acm.org/doi/10.5555/1046920.1194901},
}

@inproceedings{Lalchand20,
  title     = {Approximate Inference for Fully Bayesian Gaussian Process Regression},
  author    = {Lalchand, Vidhi and Rasmussen, Carl Edward},
  booktitle = {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
  pages     = {1--12},
  year      = {2020},
  editor    = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
  volume    = {118},
  series    = {Proceedings of Machine Learning Research},
  month     = {08 Dec},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v118/lalchand20a/lalchand20a.pdf},
  url       = {https://proceedings.mlr.press/v118/lalchand20a.html},
  abstract  = {Learning in Gaussian Process models occurs through the adaptation of hyperparameters of
               the mean and the covariance function. The classical approach entails maximizing the marginal
               likelihood yielding fixed point estimates (an approach called Type II maximum likelihood or ML-II).
               An alternative learning procedure is to infer the posterior over hyper-parameters in a hierarchical
               specification of GPs we call Fully Bayesian Gaussian Process Regression (GPR). This work considers two
               approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC)
               yielding a sampling based approximation and 2) Variational Inference (VI) where the posterior over
               hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian
               accounting for correlations between hyperparameters. We analyse the predictive performance for fully
               Bayesian GPR on a range of benchmark data sets.},
}

@misc{Lemercier21,
  title         = {SigGPDE: Scaling Sparse Gaussian Processes on Sequential Data},
  author        = {Maud Lemercier and Cristopher Salvi and Thomas Cass and Edwin V. Bonilla and Theodoros Damoulas and Terry Lyons},
  year          = {2021},
  eprint        = {2105.04211},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
}

@inproceedings{Liu18,
  title     = {Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression},
  author    = {Liu, Haitao and Cai, Jianfei and Wang, Yi and Ong, Yew Soon},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {3131--3140},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/liu18a/liu18a.pdf},
  url       = {https://proceedings.mlr.press/v80/liu18a.html},
  abstract  = {In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation
               models employ factorized training process and then combine predictions from distributed experts.
               The state-of-the-art aggregation models, however, either provide inconsistent predictions or
               require time-consuming aggregation process. We first prove the inconsistency of typical aggregations
               using disjoint or random data partition, and then present a consistent yet efficient aggregation
               model for large-scale GP. The proposed model inherits the advantages of aggregations, e.g.,
               closed-form inference and aggregation, parallelization and distributed computing. Furthermore,
               theoretical and empirical analyses reveal that the new aggregation model performs better due to
               the consistent predictions that converge to the true underlying function when the training size
               approaches infinity.},
}

@article{MacKenzie14,
  author   = {MacKenzie, Cameron A. and Trafalis, Theodore B. and Barker, Kash},
  title    = {A Bayesian beta kernel model for binary classification and online learning problems},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume   = {7},
  number   = {6},
  pages    = {434-449},
  keywords = {data mining, kernel, Bayesian, beta distribution, online learning},
  doi      = {10.1002/sam.11241},
  url      = {https://www.imse.iastate.edu/files/2015/10/MacKenzie-et-al-A-Bayesian-Beta-Kernel-Model-for-Binary-Classification-and-Online-Learning-Problems.pdf},
  abstract = {Abstract Recent advances in data mining have integrated kernel functions with Bayesian probabilistic
              analysis of Gaussian distributions. These machine-learning approaches can incorporate prior
              information with new data to calculate probabilistic rather than deterministic values for unknown
              parameters. This article extensively analyses a specific Bayesian kernel model that uses a kernel
              function to calculate a posterior beta distribution that is conjugate to the prior beta distribution.
              Numerical testing of the beta kernel model on several benchmark datasets reveals that this model's
              accuracy is comparable with those of the support vector machine (SVM), relevance vector machine,
              naive Bayes, and logistic regression, and the model runs more quickly than all the other algorithms
              except for logistic regression. When one class occurs much more frequently than the other class,
              the beta kernel model often outperforms other strategies to handle imbalanced datasets, including
              under-sampling, over-sampling, and the Synthetic Minority Over-Sampling Technique. If data arrive
              sequentially over time, the beta kernel model easily and quickly updates the probability distribution,
              and this model is more accurate than an incremental SVM algorithm for online learning.},
  year     = {2014},
}

@misc{Maddox21,
  author = {Wesley Maddox and Geoff Pleiss},
  title  = {Exact GP Regression on Classification Labels},
  url    = {https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_on_Classification_Labels.html},
}

@inproceedings{Mchutchon11,
  author    = {Mchutchon, Andrew and Rasmussen, Carl},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Gaussian Process Training with Input Noise},
  url       = {https://proceedings.neurips.cc/paper/2011/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},
  volume    = {24},
  year      = {2011},
}

@inproceedings{Milios18,
  author    = {Milios, Dimitrios and Camoriano, Raffaello and Michiardi, Pietro and Rosasco, Lorenzo and Filippone, Maurizio},
  title     = {Dirichlet-Based Gaussian Processes for Large-Scale Calibrated Classification},
  year      = {2018},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {This paper studies the problem of deriving fast and accurate classification algorithms
               with uncertainty quantification. Gaussian process classification provides a principled
               approach, but the corresponding computational burden is hardly sustainable in large-scale
               problems and devising efficient alternatives is a challenge. In this work, we investigate
               if and how Gaussian process regression directly applied to classification labels can
               be used to tackle this question. While in this case training is remarkably faster,
               predictions need to be calibrated for classification and uncertainty estimation. To
               this aim, we propose a novel regression approach where the labels are obtained through
               the interpretation of classification labels as the coefficients of a degenerate Dirichlet
               distribution. Extensive experimental results show that the proposed approach provides
               essentially the same accuracy and uncertainty quantification as Gaussian process classification
               while requiring only a fraction of computational resources.},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages     = {6008-6018},
  numpages  = {11},
  location  = {Montreal, Canada},
  series    = {NIPS'18},
  url       = {https://dl.acm.org/doi/10.5555/3327345.3327500},
}

@book{Owen13,
  author = {Art B. Owen},
  year   = {2013},
  title  = {Monte Carlo theory, methods and examples},
  url    = {https://artowen.su.domains/mc/},
  publisher = {\url{https://artowen.su.domains/mc/}},
}

@book{Rasmussen06,
  title     = {Gaussian Processes for Machine Learning},
  author    = {Rasmussen, C.E. and Williams, C.K.I.},
  isbn      = {9780262182539},
  lccn      = {20050533},
  series    = {Adaptive computation and machine learning},
  url       = {https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning},
  year      = {2006},
  publisher = {MIT Press},
}

@article{Rios19,
  title    = {Compositionally-warped Gaussian processes},
  journal  = {Neural Networks},
  volume   = {118},
  pages    = {235-246},
  year     = {2019},
  issn     = {0893-6080},
  doi      = {10.1016/j.neunet.2019.06.012},
  url      = {https://arxiv.org/abs/1906.09665},
  author   = {Gonzalo Rios and Felipe Tobar},
  keywords = {Warped Gaussian processes, Gaussian process, Non-Gaussian models, Function compositions, Neural networks},
  abstract = {The Gaussian process (GP) is a nonparametric prior distribution over functions indexed by
              time, space, or other high-dimensional index set. The GP is a flexible model yet its limitation
              is given by its very nature: it can only model Gaussian marginal distributions. To model
              non-Gaussian data, a GP can be warped by a nonlinear transformation (or warping) as performed
              by warped GPs (WGPs) and more computationally-demanding alternatives such as Bayesian WGPs
              and deep GPs. However, the WGP requires a numerical approximation of the inverse warping for
              prediction, which increases the computational complexity in practice. To sidestep this issue,
              we construct a novel class of warpings consisting of compositions of multiple elementary functions,
              for which the inverse is known explicitly. We then propose the compositionally-warped GP (CWGP),
              a non-Gaussian generative model whose expressiveness follows from its deep compositional
              architecture, and its computational efficiency is guaranteed by the analytical inverse warping.
              Experimental validation using synthetic and real-world datasets confirms that the proposed CWGP
              is robust to the choice of warpings and provides more accurate point predictions, better trained
              models and shorter computation times than WGP.},
}

@inproceedings{Titsias09,
  title     = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  author    = {Titsias, Michalis},
  booktitle = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
  pages     = {567--574},
  year      = {2009},
  editor    = {van Dyk, David and Welling, Max},
  volume    = {5},
  series    = {Proceedings of Machine Learning Research},
  address   = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month     = {16--18 Apr},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
  url       = {https://proceedings.mlr.press/v5/titsias09a.html},
  abstract  = {Sparse Gaussian process methods that use inducing variables require the selection of the
               inducing inputs and the kernel hyperparameters. We introduce a variational formulation for
               sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters
               by maximizing a lower bound of the true log marginal likelihood. The key property of this
               formulation is that the inducing inputs  are defined to be variational parameters  which are
               selected by minimizing  the Kullback-Leibler divergence between  the variational distribution
               and the exact posterior distribution over the latent function values. We apply this technique
               to regression and we compare it with other approaches in the literature.},
}

@misc{Trends,
  author = {Google},
  title  = {Google Trends data},
  url    = {https://trends.google.com/trends/},
}

@book{Wackernagel03,
  title     = {Multivariate Geostatistics: An Introduction with Applications},
  author    = {Wackernagel, Hans},
  isbn      = {9783662052945},
  url       = {https://link.springer.com/book/10.1007/978-3-662-05294-5},
  year      = {2003},
  publisher = {Springer Berlin Heidelberg},
}

@misc{FanaeeT2013,
  author       = {Fanaee-T,Hadi},
  title        = {{Bike Sharing}},
  year         = {2013},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5W894}
}

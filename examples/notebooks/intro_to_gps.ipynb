{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e112ee9",
   "metadata": {},
   "source": [
    "# Introduction to Gaussian Processes\n",
    "\n",
    "This notebook will introduce the theory behind Gaussian processes, and showcase some of the implementations of them in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d134d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "from vanguard.datasets.synthetic import SyntheticDataset, very_complicated_f\n",
    "from vanguard.kernels import ScaledRBFKernel\n",
    "from vanguard.vanilla import GaussianGPController"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63619416",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Suppose we have some data made up of input ($x$) and output ($y$) values, and we wish to fit a model which will allow us to predict new outputs in future. This is a *regression* problem, and our model takes the form of the function $f$ in the following equation:\n",
    "\n",
    "$$y_i = f(x_i) + \\epsilon_i,$$\n",
    "\n",
    "where we assume that $\\epsilon_i\\sim N(0, \\sigma_i^2)$. In linear regression, we might assume that $f$ is of the form:\n",
    "\n",
    "$$f(x_i) = ax_i + b.$$\n",
    "\n",
    "We can often get a better fit by including more terms, adding an $x_i^2$ coefficient and so on, but this still has its limits.  In fact, we can add many complex terms in an attempt to make our model as general as possible, but this becomes complicated and is ultimately very fragile. Being Bayesian, we want to place *priors* over our unknowns and infer *posteriors* using the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34322175",
   "metadata": {},
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "Gaussian processes (GPs) provide the probability distribution we require over functions. \"Gaussian\" refers to the fact that values at any point follow some normal distribution, to be inferred. GP regression is extremely flexible as one's assumptions about the unknown function $f$ are at a higher level of abstraction than in other methods.\n",
    "\n",
    "Having determined the posterior, you can obtain predictions for $f(x)$ at any collection of $x$ values. These predictions are **not** just point estimates but full probability distributions, meaning that confidence estimates can be obtained. For any input you receive not just the prediction, but a measure of how *sure* the model is about that prediction.\n",
    "\n",
    "To summarise, the main advantages of GP regression models are:\n",
    "\n",
    "* Observation uncertainty on $y$-values is quantified. If you are unsure about your true $y$-values to begin with, then the GP will take this into account.\n",
    "* Extrapolation uncertainty is quantified. The further away the point you wish to predict is from truth data, the less sure your model will be about its prediction.\n",
    "* Flexible posteriors, allowing you to include more upfront information about the expected behaviour of the model.\n",
    "* Predictions come with full probability distributions for uncertainty quantification.\n",
    "\n",
    "### Theoretical Shortcomings\n",
    "\n",
    "Standard GP regression is not a panacea for regression problems, and one should take the following into account:\n",
    "\n",
    "* Predictions must be normally distributed, which may be at odds with what you know about the data (e.g., always positive).\n",
    "* Uncertainty on the input ($x$) values cannot be handled, meaning that the model may be more sure about a prediction than it ought to be.\n",
    "* Exact inference scales cubicly with dataset size. This means that for $n$ data points, the inference runs in $\\mathcal{O}(n^3)$ time, which can get very slow very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855e916",
   "metadata": {},
   "source": [
    "The Mathematics Behind Gaussian Processes\n",
    "-----------------------------------------\n",
    "\n",
    "Gaussian processes are not an easy thing to understand, and the allusion to linear and multinomial regression can only carry the reader so far. The following explanation is taken from Chapter 2 of :cite:`Rasmussen06`, which although a thorough exploration of the theory, does not make for light reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a056fb2",
   "metadata": {},
   "source": [
    "A Gaussian process is completely specified by its *mean function* and *covariance function*. These are denoted by $m(x)$ and $k(x, x')$ respectively, and defined as follows:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    m(x) &= E[f(x)], \\\\\n",
    "    k(x, x') &= E[(f(x) - m(x))(f(x') - m(x'))].\n",
    "\\end{align} $$\n",
    "\n",
    "Note that the covariance between the *outputs* is written as a function of the *inputs*. We write the Gaussian process as a distribution not dissimilar to a Gaussian distribution, except instead of being over the real numbers it is over the *function space* over the reals:\n",
    "\n",
    "$$ f \\sim GP(m(x), k(x, x')). $$\n",
    "\n",
    "A common choice for the mean function is the *zero function*:\n",
    "\n",
    "$$m(x) = 0,$$\n",
    "\n",
    "and a common choice for the covariance function is the *squared exponential* function:\n",
    "\n",
    "$$k(x, x') = e^{-\\frac{1}{2}|x-x'|^2}.$$\n",
    "\n",
    "(Other choices are available, and when to use them depends on the type of data you have, and behaviours you expect from your model.)  As the distance between $x$ and $x'$ decreases to zero, the covariance approaches 1, and as the distance increases to infinity, the covariance approaches zero. This makes sense, since the effect of a given data point on our truth data should depend somewhat on how close we are to that data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ccbb9",
   "metadata": {},
   "source": [
    "In order to understand the mathematics, it helps to consider the case where we have a zero mean, and there is no noise on the observations, i.e. the training data we have is exact with no uncertainty on the $y$-values. Consider the model $f$ we fit over the $n$ observations (our training data) not as a function per se, but as a *distribution* from which we draw said function. Suppose we wish to predict the $y$-values over $n_*$ test data points, then the *joint distribution* of the training outputs ($f$) and the test outputs ($f*$) with respect to the prior is:\n",
    "\n",
    "$$ \\begin{bmatrix}f\\\\f_*\\end{bmatrix} = N\\left(0, \\begin{bmatrix}K(X,X) & K(X,X_*)\\\\ K(X_*,X) & K(X_*,X_*)\\end{bmatrix}\\right), $$\n",
    "\n",
    "where $K(X,X_*)$ denotes the $n\\times n_*$ matrix of the covariances calculated at all pairs of training and test points (subject to our covariance function $k$), and similar for all other $K(\\cdot,\\cdot)$. To infer the posterior distribution we also restrict this joint prior distribution to function which will concur with the observations. We do this by *conditioning* the joint prior on the observations using standard Gaussian conditioning rules, which gives the following formula for our $f_*$ distribution:\n",
    "\n",
    "$$ f_*|X_*,X,f \\sim N\\left(M(X,X_*), \\Psi(X,X_*)\\right), $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\begin{align}\n",
    "    M(X,X_*) &= K(X_*,X)K(X,X)^{-1}f, \\\\\n",
    "    \\Psi(X,X_*) &= K(X_*,X_*)-K(X_*,X)K(X,X)^{-1}K(X,X_*).\n",
    "\\end{align} $$\n",
    "\n",
    "Function values can then be sampled from this distribution, which will yield our *model* for the test values. This is how prediction works for a Gaussian process, and explains the uncertainty that the model includes with it. Note the required inverse of the $K(X, X)$ matrix, which explains the cubic complexity of the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c20f8",
   "metadata": {},
   "source": [
    "Python Implementations of GPs\n",
    "-----------------------------\n",
    "\n",
    "Many data science and machine learning libraries exist which can implement simple Gaussian processes. The data we will be using is the :class:`~vanguard.datasets.synthetic.SyntheticDataset`, with the :func:`vanguard.datasets.synthetic.very_complicated_f` function, given by:\n",
    "\n",
    ".. math::\n",
    "    f(x) = -x^\\frac{3}{2} + x\\sin^2(2\\pi x) + x^2 \\cos(10\\pi x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = SyntheticDataset(functions=[very_complicated_f], n_train_points=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275c8f1",
   "metadata": {},
   "source": [
    "Sci-kit Learn\n",
    "--------------\n",
    "\n",
    "The most straightforward implementation is in `sklearn`, with a trademark light-weight API. We first need a *kernel*, which is the Python equivalent which implements the covariance function :math:`k`. The :class:`~sklearn.gaussian_process.kernels.RBF` kernel is the implementation of the squared exponential covariance function mentioned above. We compose it with a class:`~sklearn.gaussian_process.kernels.ConstantKernel`, to allow us to scale the covariance to accommodate the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ConstantKernel() * RBF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98444b2",
   "metadata": {},
   "source": [
    "We can then instantiate our model with the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, alpha=DATASET.train_y_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29c934",
   "metadata": {},
   "source": [
    "As is common with ``sklearn`` we call the :meth:`~sklearn.gaussian_process.GaussianProcessRegressor.fit` method, which will tune the hyperparameters in the mean and the kernel of the Gaussian process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(DATASET.train_x, DATASET.train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e10928",
   "metadata": {},
   "source": [
    "Our predictions can then be pulled from the model using the :meth:`~sklearn.gaussian_process.GaussianProcessRegressor.predict` method. By predicting across a mesh of points over the training data, we can see the effects of the uncertainty on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linspace = np.linspace(DATASET.train_x.min(), DATASET.train_x.max(), num=100)\n",
    "predictions, uncertainty = gp.predict(linspace.reshape(-1, 1), return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(DATASET.train_x, DATASET.train_y, label=\"Truth\")\n",
    "plt.plot(linspace, predictions, color=\"olive\", label=\"Prediction\")\n",
    "plt.fill_between(linspace, predictions - 1.96 * uncertainty, predictions + 1.96 * uncertainty, color=\"olive\", alpha=0.3)\n",
    "plt.title(\"Sci-kit Learn Gaussian Process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d83228",
   "metadata": {},
   "source": [
    "GPyTorch\n",
    "--------\n",
    "\n",
    "Sci-kit learn is a great choice for implementing a quick GP, but it has very little room for adjustment and it is almost impossible to do anything advanced. On the other side of the spectrum, ``gpytorch`` allows for an almost unbounded set of features by fully exposing all parts of the GP architecture. Our initial kernel is identical to the above example, composing the :class:`~gpytorch.kernels.RBFKernel` with a class:`~gpytorch.kernels.ScaleKernel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9944d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901abf4",
   "metadata": {},
   "source": [
    "We also now have freedom over the mean. A good starting place is the class:`~gpytorch.means.ConstantMean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab299c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = gpytorch.means.ConstantMean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19e820",
   "metadata": {},
   "source": [
    "Unlike ``sklearn``, we need to specify a likelihood function, which controls the mapping from the image of points under a function (:math:`f(x)`) to the labels. This can be as simple as adding some Gaussian noise as in the class:`~gpytorch.likelihoods.GaussianLikelihood`, or something more complex to enable binary classification as in the class:`~gpytorch.likelihoods.BernoulliLikelihood`. However, the former is the best choice for a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e833d1c",
   "metadata": {},
   "source": [
    "Finally, we need to build a model class to handle the inference. GPyTorch makes you do this yourself to allow you full control, but it can seem like a daunting task. For a simple case, one need only subclass the class:`~gpytorch.models.ExactGP` class, and specify a ``forward`` method to pass data through the mean and covariance functions. The class:`~gpytorch.distributions.MultivariateNormal` instance is the standard output for these functions, allowing us to work with a distribution directly instead of separating the mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "\n",
    "    def __init__(self, train_x: Tensor, train_y: Tensor, y_std: Tensor) -> None:\n",
    "        likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=y_std)\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x: Tensor) -> gpytorch.distributions.MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45619c92",
   "metadata": {},
   "source": [
    "We can then instantiate our model. Note that GPyTorch works with tensors instead of numpy arrays, so we need to ensure that we convert them before passing them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b53cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = ExactGPModel(torch.as_tensor(DATASET.train_x), torch.as_tensor(DATASET.train_y),\n",
    "                  torch.ones(len(DATASET.train_y)) * DATASET.train_y_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1867ac",
   "metadata": {},
   "source": [
    "Fitting the model is a much more manual task here than it was before, requiring us to build our own function. We make use of the class:`~torch.optim.Adam` optimiser, and also the class:`~gpytorch.mlls.ExactMarginalLogLikelihood` class. This will compute the marginal log likelihood of the model when applied to some data, and can be turned into \"loss\" functions by negating them. There are a few variations, but this one is sufficient for this simple case. We wrap all of these into a ``fit`` function to broadly emulate the one from ``sklearn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam([{\"params\": gp.parameters()}], lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "\n",
    "\n",
    "def fit(model: ExactGPModel, train_x: Tensor, train_y: Tensor, n_iters: int) -> None:\n",
    "    model.train()\n",
    "    model.likelihood.train()\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        optimiser.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(gp, torch.as_tensor(DATASET.train_x), torch.as_tensor(DATASET.train_y), n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47ea0e",
   "metadata": {},
   "source": [
    "Again, we are expected to craft our own `predict` method, which is much simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: ExactGPModel, x: Tensor) -> Tuple[NDArray, NDArray]:\n",
    "    model.eval()\n",
    "    model.likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        prediction = model.likelihood(model(x))\n",
    "\n",
    "    means = prediction.loc.cpu()\n",
    "    variances = prediction.lazy_covariance_matrix.diag().cpu()\n",
    "\n",
    "    return means, np.sqrt(np.abs(variances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf68e70",
   "metadata": {},
   "source": [
    "As before, we can see how well the model incorporates uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b31bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, uncertainty = predict(gp, torch.as_tensor(linspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad056af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(DATASET.train_x, DATASET.train_y, label=\"Truth\")\n",
    "plt.plot(linspace, predictions, color=\"green\", label=\"Prediction\")\n",
    "plt.fill_between(linspace, predictions - 1.96 * uncertainty, predictions + 1.96 * uncertainty, color=\"green\", alpha=0.3)\n",
    "plt.title(\"GPyTorch Gaussian Process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919e98b",
   "metadata": {},
   "source": [
    "Vanguard\n",
    "--------\n",
    "\n",
    "Finally, we come to Vanguard, which is built upon GPyTorch to allow advanced GP functionality without requiring too much knowledge from the user. Given its base, Vanguard can be adjusted to use many components from GPyTorch, and also allows for easy composability of the advanced features made available. Instead of composing our own kernels (which is still possible) we use the specific class:`~vanguard.kernels.ScaledRBFKernel`. Note that components are passed around in Vanguard as *uninstantiated classes* instead of instances, which enables much of the composability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8222b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianGPController(DATASET.train_x, DATASET.train_y, kernel_class=ScaledRBFKernel, y_std=DATASET.train_y_std)\n",
    "gp.fit(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfba08",
   "metadata": {},
   "source": [
    "Instead of a ``predict`` method, Vanguard opts for the :meth:`~vanguard.base.gpcontroller.GPController.posterior_over_point` method, which will return an instance of a class:`~vanguard.base.posteriors.Posterior` class, allowing some of the features to alter how the distribution leads to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.predictive_likelihood(linspace)\n",
    "predictions, covar = posterior._tensor_prediction()\n",
    "predictions, covar = predictions.cpu(), covar.cpu()\n",
    "uncertainty = np.sqrt(covar.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79175189",
   "metadata": {},
   "source": [
    "To simplify matters, one can just call the :meth:`~vanguard.base.posteriors.Posterior.confidence_interval` method instead of working directly with the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "median, lower, upper = posterior.confidence_interval()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(DATASET.train_x, DATASET.train_y, label=\"Truth\")\n",
    "plt.plot(linspace, median, color=\"red\", label=\"Prediction\")\n",
    "plt.fill_between(linspace, lower, upper, color=\"red\", alpha=0.3)\n",
    "plt.title(\"Vanguard Gaussian Process\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4de7eb",
   "metadata": {},
   "source": [
    "This is arguably the best fit, as the uncertainty around the target values is properly accounted for, yet the mean still follows the overall trend of the points. Vanguard also scales its inputs by default, which often leads to faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996df7fa",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook we have introduced the theory of Gaussian processes, and compared some common Python implementations. In order to see how advanced features can be applied using Vanguard, have a look through some other example notebooks for specific examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

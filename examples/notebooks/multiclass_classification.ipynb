{
 "cells": [
  {
   "cell_type": "raw",
   "id": "472de2d910865e03",
   "metadata": {},
   "source": [
    "Multiclass Classification in Vanguard\n",
    "-------------------------------------\n",
    "\n",
    "A showcase of the implementation of standard multiclass classification in Vanguard. This builds upon the `binary classification example <binary_classification.ipynb>`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.mlls import VariationalELBO\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vanguard.classification import CategoricalClassification\n",
    "from vanguard.classification.likelihoods import MultitaskBernoulliLikelihood, SoftmaxLikelihood\n",
    "from vanguard.datasets.classification import MulticlassGaussianClassificationDataset\n",
    "from vanguard.kernels import ScaledRBFKernel\n",
    "from vanguard.multitask import Multitask\n",
    "from vanguard.vanilla import GaussianGPController\n",
    "from vanguard.variational import VariationalInference"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62e7396b8ac9bce3",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "As seen in the `binary classification example notebook <binary_classification.ipynb>`_, classification can rephrased as a regression problem fairly straightforwardly by treating class labels as points in the interval $[0, 1]$. At its most basic, multiclass classification can be thought of as an aggregation of individual binary classifiers for each class label, giving a score for each class. However, this is hardly a robust and principled solution. Firstly, the subsequent \"probabilities\" collected from each class predictor are unlikely to sum to 1. Secondly, this method would treat each class as independent which can be a dangerous assumption and hurt model accuracy. Luckily it is possible to overcome both of these problems with standard components from Vanguard and GPyTorch."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa67a9d2",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "We start with the :py:class:`~vanguard.datasets.classification.MulticlassGaussianClassificationDataset` for this experiment, which creates multiple classes based on the distance to the centre of a two-dimensional Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "\n",
    "DATASET = MulticlassGaussianClassificationDataset(\n",
    "    num_train_points=100, num_test_points=500, num_classes=NUM_CLASSES, covariance_scale=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5375b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc14f049",
   "metadata": {},
   "source": [
    "Binary Multitask Modelling\n",
    "--------------------------\n",
    "\n",
    "It is possible to overcome the issue of class dependence using the :py:class:`~vanguard.multitask.decorator.Multitask` decorator, which tracks covariance between multiple regressors. Instead of using the :py:class:`~vanguard.classification.binary.BinaryClassification` decorator directly, we instead use the :py:class:`~vanguard.classification.categorical.CategoricalClassification` decorator. Finally, recall that binary classification requires a :py:class:`~gpytorch.likelihoods.BernoulliLikelihood` to properly run inference. For multitask, we need to use the corresponding :py:class:`~vanguard.classification.likelihoods.MultitaskBernoulliLikelihood`, which will sum the log probabilities over the task dimension. As before, this requires us to use the :py:class:`~vanguard.variational.VariationalInference` decorator to enable approximate inference.\n",
    "\n",
    "As described in the introduction, this initial simplistic approach uses Bernoulli likelihoods for each task of a multitask GP. As mentioned, the resulting likelihood is improper, in that the probabilities for each class do not sum to 1, but the the \"probabilities\" for each class can be interpreted as the model's confidence in that class. The closest analogy would be a more probabilistically principled version of the logits of a neural network classifier. Note that the tasks are not completely independent, due to the multitask GP structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@CategoricalClassification(num_classes=NUM_CLASSES, ignore_all=True)\n",
    "@Multitask(num_tasks=NUM_CLASSES, ignore_all=True)\n",
    "@VariationalInference(ignore_all=True)\n",
    "class CategoricalMultitaskClassifier(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "549e8d80",
   "metadata": {},
   "source": [
    "We will use a :py:class:`~vanguard.kernels.ScaledRBFKernel`. Note that we do not pass the model the standard ``train_y``, but the special ``one_hot_train_y`` which encodes the target class labels into one-hot vectors to more easily enable multitask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = CategoricalMultitaskClassifier(\n",
    "    DATASET.train_x,\n",
    "    DATASET.one_hot_train_y,\n",
    "    ScaledRBFKernel,\n",
    "    y_std=0,\n",
    "    likelihood_class=MultitaskBernoulliLikelihood,\n",
    "    marginal_log_likelihood_class=VariationalELBO,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a68a2eda",
   "metadata": {},
   "source": [
    "Before we try fitting, let's see how well the classifier does without any hyperparameter training. We cannot use the :py:meth:`~vanguard.base.gpcontroller.GPController.posterior_over_point` method, as the model posteriors need to be passed through the likelihood. Instead, classifiers in Vanguard have a special ``classify_points`` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd4d5dd9a9f7072",
   "metadata": {},
   "source": [
    "In the above plot, the fill colour of a point denotes the predicted class, whereas the edge colour denotes the correct class.  As we can plainly see, the model isn't very good without being trained.  However, a small amount of fitting will improve things immensely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca766f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.fit(100)\n",
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c63d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61c993b3713efe80",
   "metadata": {},
   "source": [
    "It may be helpful to look at a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_confusion_matrix(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c807ba9b",
   "metadata": {},
   "source": [
    "The default variational strategy for multitask GPs approximates the posterior as entirely independent single-task GPs. There is an alternative, namely linear model co-regionalisation (LMC) :cite:`Wackernagel03`, which can be used simply by providing the number of latent dimensions to the :py:class:`~vanguard.multitask.decorator.Multitask` decorator. The resulting models should be able to achieve superior classification accuracies when trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6996d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LATENTS = 10\n",
    "\n",
    "\n",
    "@CategoricalClassification(num_classes=NUM_CLASSES, ignore_all=True)\n",
    "@Multitask(num_tasks=NUM_CLASSES, lmc_dimension=NUM_LATENTS, ignore_all=True)\n",
    "@VariationalInference(ignore_all=True)\n",
    "class CategoricalMultitaskClassifier(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ace0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = CategoricalMultitaskClassifier(\n",
    "    DATASET.train_x,\n",
    "    DATASET.one_hot_train_y,\n",
    "    ScaledRBFKernel,\n",
    "    y_std=0,\n",
    "    likelihood_class=MultitaskBernoulliLikelihood,\n",
    "    marginal_log_likelihood_class=VariationalELBO,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d40a67a",
   "metadata": {},
   "source": [
    "Again, let's see how well the classifier does without any hyperparameter training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc34e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.fit(100)\n",
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77399c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b836b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_confusion_matrix(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33785bd7a8f2dc9b",
   "metadata": {},
   "source": [
    "LMC is not necessarily immediately superior to the simpler strategy, but with sufficient tuning we expect it to become so."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd520261",
   "metadata": {},
   "source": [
    "Softmax Multiclass Modelling\n",
    "----------------------------\n",
    "\n",
    "A more robust choice of likelihood is the :py:class:`~vanguard.classification.likelihoods.SoftmaxLikelihood`. Instead of simply summing the log probabilities, we use the `softmax function <https://en.wikipedia.org/wiki/Softmax_function>`_:\n",
    "\n",
    ".. math::\n",
    "    \\sigma(z)_i = \\dfrac{e^{z_i}}{\\sum_{j=1}^Ke^{z_j}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LATENTS = 10\n",
    "NUM_FEATURES = 6\n",
    "\n",
    "\n",
    "@CategoricalClassification(num_classes=NUM_CLASSES, ignore_all=True)\n",
    "@Multitask(num_tasks=NUM_FEATURES, lmc_dimension=NUM_LATENTS, ignore_all=True)\n",
    "@VariationalInference(ignore_all=True)\n",
    "class CategoricalSoftmaxMultitaskClassifier(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a852b4166eb228",
   "metadata": {},
   "source": [
    "When using softmax, we no longer use the one-hot encoded vectors, and instead return to the ``train_y`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = CategoricalSoftmaxMultitaskClassifier(\n",
    "    DATASET.train_x,\n",
    "    DATASET.train_y,\n",
    "    ScaledRBFKernel,\n",
    "    y_std=0,\n",
    "    likelihood_class=SoftmaxLikelihood,\n",
    "    marginal_log_likelihood_class=VariationalELBO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b00b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.fit(100)\n",
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_confusion_matrix(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98dc58a91f7e762e",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "===========\n",
    "\n",
    "Multi-class classification with Gaussian processes does not perform as well as other machine learning techniques like neural networks, but it does require fewer parameters to get decent results. Although larger amounts of data will not scale well, the use of variational inference allows us to mitigate this somewhat. Ultimately, the contents of this notebook act more as a research showcase than a recommended method for classification, but more advanced features for scalability (such as `distributed GPs <distributed_gp.ipynb>`_) could lead to more plausible use cases in the future."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

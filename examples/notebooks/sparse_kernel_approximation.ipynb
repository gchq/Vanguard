{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a54515db",
   "metadata": {},
   "source": [
    "Sparse GP regression\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b124c5d79cd6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â© Crown Copyright GCHQ\n",
    "#\n",
    "# Licensed under the GNU General Public License, version 3 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.gnu.org/licenses/gpl-3.0.en.html\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef607c9733c8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is not compiled into the documentation due to the time taken to run it to get\n",
    "# a representative analysis. Please run this notebook locally if you wish to see the outputs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "70547e65b981ef05",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate use of sparse inducing point kernel approximations :cite:`Titsias09` within Vanguard, including combining with warping or input uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1321f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef681e8370f7f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1_989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc986a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from vanguard.datasets.bike import BikeDataset\n",
    "from vanguard.models import InducingPointKernelGPModel\n",
    "from vanguard.uncertainty import GaussianUncertaintyGPController\n",
    "from vanguard.vanilla import GaussianGPController\n",
    "from vanguard.warps import SetWarp, warpfunctions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e680230",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "We will use the :py:class:`~vanguard.datasets.bike.BikeDataset`, with 13 input features. The main point of this dataset for this example notebook is that it's large. With a 90/10 train/test split, we have ~15.5k training points. Exact GP inference in this case would be very expensive. This dataset is taken from :cite:`FanaeeT2013` and was accessed and copied to Github LFS within this repo on 1st July 2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = BikeDataset(rng=np.random.default_rng(random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b743c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DATASET.train_y.numpy(force=True))\n",
    "plt.xlabel(\"$y$\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f7179cf169396c1",
   "metadata": {},
   "source": [
    "The regressand is non-negative, so warping could be useful. Let's standardise the inputs for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7392b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_train_x = scaler.fit_transform(DATASET.train_x.numpy(force=True))\n",
    "scaled_test_x = scaler.transform(DATASET.test_x.numpy(force=True))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cac3d1f1",
   "metadata": {},
   "source": [
    "Modelling\n",
    "---------\n",
    "\n",
    "Let's try a vanilla GP using a sparse kernel approx. To do this, we just have to use :py:class:`~vanguard.models.InducingPointKernelGPModel` with a plain :py:class:`~vanguard.vanilla.GaussianGPController`. In order to use a different model, we need a new controller subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGaussianGPController(GaussianGPController):\n",
    "    gp_model_class = InducingPointKernelGPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72137187",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INDUCING_POINTS = 50\n",
    "num_iters = int(len(scaled_train_x) / 64) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e003dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledMaternKernel(ScaleKernel):\n",
    "    \"\"\"A scaled matern kernel.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(MaternKernel(nu=1.5, ard_num_dims=scaled_train_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = SparseGaussianGPController(\n",
    "    train_x=scaled_train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    gp_kwargs={\"n_inducing_points\": N_INDUCING_POINTS},\n",
    "    optim_kwargs={\"lr\": 0.01},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "\n",
    "gp.fit(n_sgd_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9ccce-bd64-40ca-8b9f-4c46924d6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(scaled_test_x)\n",
    "mean, lower, upper = posterior.confidence_interval()\n",
    "\n",
    "# Convert to numpy arrays for plotting\n",
    "plt_test_y = DATASET.test_y.numpy(force=True)\n",
    "mean = mean.numpy(force=True)\n",
    "lower = lower.numpy(force=True)\n",
    "upper = upper.numpy(force=True)\n",
    "\n",
    "print(f\"RMSE: {np.sqrt(np.mean((plt_test_y - mean) ** 2))}\")\n",
    "plt.errorbar(plt_test_y, mean, yerr=np.vstack([mean - lower, upper - mean]), marker=\"o\", label=\"mean\", linestyle=\"\")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "886ec5f3",
   "metadata": {},
   "source": [
    "Now let's look at sparse kernels combined with compositional warping. We'll use an affine-log warp to reflect the non-negativity of the data: :math:`\\phi(y) = a + b\\log(y)`. The code to create this GP model in Vanguard is simple. Use a :py:class:`~vanguard.warps.SetWarp` decorator to apply the warp to the same controller used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = warpfunctions.AffineWarpFunction() @ warpfunctions.BoxCoxWarpFunction(lambda_=0)\n",
    "\n",
    "\n",
    "@SetWarp(warp_function=warp, ignore_methods=(\"fit\", \"__init__\"))\n",
    "class WarpedGaussianGPController(SparseGaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = WarpedGaussianGPController(\n",
    "    train_x=scaled_train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    gp_kwargs={\"n_inducing_points\": N_INDUCING_POINTS},\n",
    "    optim_kwargs={\"lr\": 0.01},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "\n",
    "gp.fit(n_sgd_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d83020-0250-49d6-8ff6-ed21c4adf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(scaled_test_x)\n",
    "warp_mean, warp_lower, warp_upper = posterior.confidence_interval()\n",
    "\n",
    "# Convert to numpy arrays for plotting\n",
    "warp_mean = warp_mean.numpy(force=True)\n",
    "warp_lower = warp_lower.numpy(force=True)\n",
    "warp_upper = warp_upper.numpy(force=True)\n",
    "\n",
    "\n",
    "print(f\"RMSE: {np.sqrt(np.mean((plt_test_y - warp_mean) ** 2))}\")\n",
    "plt.errorbar(\n",
    "    plt_test_y,\n",
    "    mean,\n",
    "    yerr=np.vstack([warp_mean - warp_lower, warp_upper - warp_mean]),\n",
    "    marker=\"o\",\n",
    "    label=\"mean\",\n",
    "    linestyle=\"\",\n",
    ")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0a33d4fe2c290af",
   "metadata": {},
   "source": [
    "Warping here is likely to be most useful for smaller :math:`y` values, so let's filter by the true :math:`y` value and compare warping to no warping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "small_indices = plt_test_y < 0.5\n",
    "\n",
    "plt.errorbar(\n",
    "    plt_test_y[small_indices],\n",
    "    mean[small_indices],\n",
    "    yerr=np.vstack([mean - lower, upper - mean])[:, small_indices],\n",
    "    marker=\"o\",\n",
    "    label=\"mean\",\n",
    "    linestyle=\"\",\n",
    ")\n",
    "plt.title(f\"No warping. RMSE: \" f\"{np.sqrt(np.mean((plt_test_y[small_indices] - mean[small_indices]) ** 2)):.4}\")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_err = np.vstack([warp_mean - warp_lower, warp_upper - warp_mean])[:, small_indices]\n",
    "plt.errorbar(plt_test_y[small_indices], warp_mean[small_indices], yerr=y_err, marker=\"o\", label=\"mean\", linestyle=\"\")\n",
    "plt.title(\n",
    "    f\"Affine-log warping. RMSE: {np.sqrt(np.mean((plt_test_y[small_indices] - warp_mean[small_indices]) ** 2)):.4}\"\n",
    ")\n",
    "plt.xlabel(\"true y values\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cec19a6bbae3e1ff",
   "metadata": {},
   "source": [
    "This demonstrates nicely that the warping is working where it matters, preventing impossible negative predictions.\n",
    "\n",
    "Finally we can demonstrate combining with input uncertainty as well, using some dummy input noise. Variational inference in batch mode is not yet supported with input uncertainty, so we'll subset the data and switch-off batch-mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = warpfunctions.AffineWarpFunction() @ warpfunctions.BoxCoxWarpFunction(lambda_=0)\n",
    "\n",
    "\n",
    "@SetWarp(warp_function=warp, ignore_all=True)\n",
    "class WarpedGaussianUncertaintyGPController(GaussianUncertaintyGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = WarpedGaussianUncertaintyGPController(\n",
    "    train_x=scaled_train_x[:500],\n",
    "    train_x_std=0.1,\n",
    "    train_y=DATASET.train_y[:500],\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=0.001 * torch.mean(torch.abs(DATASET.train_y)),\n",
    "    likelihood_kwargs={\"learn_additional_noise\": True},\n",
    "    batch_size=None,\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "gp.fit(n_sgd_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf879eb-8635-45af-b6c4-9e3e65ca734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(scaled_test_x)\n",
    "mean, lower, upper = posterior.confidence_interval()\n",
    "\n",
    "# Convert to numpy arrays for plotting\n",
    "mean = mean.numpy(force=True)\n",
    "lower = lower.numpy(force=True)\n",
    "upper = upper.numpy(force=True)\n",
    "\n",
    "print(f\"RMSE: {np.sqrt(np.mean((plt_test_y - mean) ** 2))}\")\n",
    "plt.errorbar(plt_test_y, mean, yerr=np.vstack([mean - lower, upper - mean]), marker=\"o\", label=\"mean\", linestyle=\"\")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "943da7da4e8c498e",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "-----------\n",
    "\n",
    "This short example demonstrates that compositional warping can be combined with sparse kernel approximations in Vanguard using very little code. We have demonstrated good results on a real-world dataset with ~15.5k training items. Other datasets may exhibit a stronger preference for warping, but we have shown that, for low values of the regressand (close to zero), the warped GP is much better, as it makes no impossible negative predictions. We have shown that combining warping and sparse kernel approximations is feasible and the training is no more difficult than without the warping.\n",
    "\n",
    "In addition, we have provided a proof-of-concept demonstration that warping, input uncertainty and  sparse kernel approximations can be combined simply within Vanguard and trained successfully."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a54515db",
   "metadata": {},
   "source": [
    "Sparse GP regression\n",
    "====================\n",
    "\n",
    "In this notebook we demonstrate use of sparse inducing point kernel approximations :cite:`Titsias09` within Vanguard, including combining with warping or input uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1321f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc986a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from vanguard.datasets.bike import BikeDataset\n",
    "from vanguard.models import InducingPointKernelGPModel\n",
    "from vanguard.uncertainty import GaussianUncertaintyGPController\n",
    "from vanguard.vanilla import GaussianGPController\n",
    "from vanguard.warps import SetWarp, warpfunctions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e680230",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "We will use the :py:class:`~vanguard.datasets.bike.BikeDataset`, with 13 input features. The main point of this dataset for this example notebook is that it's large. With a 90/10 train/test split, we have ~15.5k training points. Exact GP inference in this case would be very expensive. This dataset is taken from :cite:`FanaeeT2013` and was accessed and copied to Github LFS within this repo on 1st July 2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = BikeDataset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b743c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DATASET.train_y)\n",
    "plt.xlabel(\"$y$\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f7179cf169396c1",
   "metadata": {},
   "source": [
    "The regressand is non-negative, so warping could be useful. Let's standardise the inputs for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7392b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_train_x = scaler.fit_transform(DATASET.train_x)\n",
    "scaled_test_x = scaler.transform(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cac3d1f1",
   "metadata": {},
   "source": [
    "Modelling\n",
    "---------\n",
    "\n",
    "Let's try a vanilla GP using a sparse kernel approx. To do this, we just have to use :py:class:`~vanguard.models.InducingPointKernelGPModel` with a plain :py:class:`~vanguard.vanilla.GaussianGPController`. In order to use a different model, we need a new controller subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGaussianGPController(GaussianGPController):\n",
    "    gp_model_class = InducingPointKernelGPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72137187",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INDUCING_POINTS = 50\n",
    "NUM_ITERS = int(len(scaled_train_x) / 64) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e003dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledMaternKernel(ScaleKernel):\n",
    "    \"\"\"A scaled matern kernel.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(MaternKernel(nu=1.5, ard_num_dims=scaled_train_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = SparseGaussianGPController(\n",
    "    train_x=scaled_train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    gp_kwargs={\"n_inducing_points\": N_INDUCING_POINTS},\n",
    "    optim_kwargs={\"lr\": 0.01},\n",
    ")\n",
    "\n",
    "gp.fit(n_sgd_iters=NUM_ITERS)\n",
    "\n",
    "posterior = gp.posterior_over_point(scaled_test_x)\n",
    "mean, lower, upper = posterior.confidence_interval()\n",
    "print(f\"RMSE: {np.sqrt(np.mean((DATASET.test_y - mean) ** 2))}\")\n",
    "plt.errorbar(DATASET.test_y, mean, yerr=np.vstack([mean - lower, upper - mean]), marker=\"o\", label=\"mean\", linestyle=\"\")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "886ec5f3",
   "metadata": {},
   "source": [
    "Now let's look at sparse kernels combined with compositional warping. We'll use an affine-log warp to reflect the non-negativity of the data: :math:`\\phi(y) = a + b\\log(y)`. The code to create this GP model in Vanguard is simple. Use a :py:class:`~vanguard.warps.SetWarp` decorator to apply the warp to the same controller used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = warpfunctions.AffineWarpFunction() @ warpfunctions.BoxCoxWarpFunction(lambda_=0)\n",
    "\n",
    "\n",
    "@SetWarp(warp_function=warp, ignore_methods=(\"fit\", \"__init__\"))\n",
    "class WarpedGaussianGPController(SparseGaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = WarpedGaussianGPController(\n",
    "    train_x=scaled_train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    gp_kwargs={\"n_inducing_points\": N_INDUCING_POINTS},\n",
    "    optim_kwargs={\"lr\": 0.01},\n",
    ")\n",
    "\n",
    "gp.fit(n_sgd_iters=NUM_ITERS)\n",
    "\n",
    "posterior = gp.posterior_over_point(scaled_test_x)\n",
    "warp_mean, warp_lower, warp_upper = posterior.confidence_interval()\n",
    "print(f\"RMSE: {np.sqrt(np.mean((DATASET.test_y - warp_mean) ** 2))}\")\n",
    "plt.errorbar(\n",
    "    DATASET.test_y,\n",
    "    mean,\n",
    "    yerr=np.vstack([warp_mean - warp_lower, warp_upper - warp_mean]),\n",
    "    marker=\"o\",\n",
    "    label=\"mean\",\n",
    "    linestyle=\"\",\n",
    ")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0a33d4fe2c290af",
   "metadata": {},
   "source": [
    "Warping here is likely to be most useful for smaller :math:`y` values, so let's filter by the true :math:`y` value and compare warping to no warping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "small_indices = DATASET.test_y < 0.5\n",
    "plt.errorbar(\n",
    "    DATASET.test_y[small_indices],\n",
    "    mean[small_indices],\n",
    "    yerr=np.vstack([mean - lower, upper - mean])[:, small_indices],\n",
    "    marker=\"o\",\n",
    "    label=\"mean\",\n",
    "    linestyle=\"\",\n",
    ")\n",
    "plt.title(f\"No warping. RMSE: \" f\"{np.sqrt(np.mean((DATASET.test_y[small_indices] - mean[small_indices]) ** 2)):.4}\")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "small_indices = DATASET.test_y < 0.5\n",
    "y_err = np.vstack([warp_mean - warp_lower, warp_upper - warp_mean])[:, small_indices]\n",
    "plt.errorbar(\n",
    "    DATASET.test_y[small_indices], warp_mean[small_indices], yerr=y_err, marker=\"o\", label=\"mean\", linestyle=\"\"\n",
    ")\n",
    "plt.title(\n",
    "    f\"Affine-log warping. RMSE: \"\n",
    "    f\"{np.sqrt(np.mean((DATASET.test_y[small_indices] - warp_mean[small_indices]) ** 2)):.4}\"\n",
    ")\n",
    "plt.xlabel(\"true y values\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cec19a6bbae3e1ff",
   "metadata": {},
   "source": [
    "This demonstrates nicely that the warping is working where it matters, preventing impossible negative predictions.\n",
    "\n",
    "Finally we can demonstrate combining with input uncertainty as well, using some dummy input noise. Variational inference in batch mode is not yet supported with input uncertainty, so we'll subset the data and switch-off batch-mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = warpfunctions.AffineWarpFunction() @ warpfunctions.BoxCoxWarpFunction(lambda_=0)\n",
    "\n",
    "\n",
    "@SetWarp(warp_function=warp, ignore_all=True)\n",
    "class WarpedGaussianUncertaintyGPController(GaussianUncertaintyGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = WarpedGaussianUncertaintyGPController(\n",
    "    train_x=scaled_train_x[:500],\n",
    "    train_x_std=0.1,\n",
    "    train_y=DATASET.train_y[:500],\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=0.001 * np.mean(np.abs(DATASET.train_y)),\n",
    "    likelihood_kwargs={\"learn_additional_noise\": True},\n",
    "    batch_size=None,\n",
    ")\n",
    "gp.fit(n_sgd_iters=NUM_ITERS)\n",
    "\n",
    "posterior = gp.posterior_over_point(scaled_test_x)\n",
    "mean, lower, upper = posterior.confidence_interval()\n",
    "print(f\"RMSE: {np.sqrt(np.mean((DATASET.test_y - mean) ** 2))}\")\n",
    "plt.errorbar(DATASET.test_y, mean, yerr=np.vstack([mean - lower, upper - mean]), marker=\"o\", label=\"mean\", linestyle=\"\")\n",
    "plt.xlabel(\"true y values\")\n",
    "plt.ylabel(\"predicted y values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "943da7da4e8c498e",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "-----------\n",
    "\n",
    "This short example demonstrates that compositional warping can be combined with sparse kernel approximations in Vanguard using very little code. We have demonstrated good results on a real-world dataset with ~15.5k training items. Other datasets may exhibit a stronger preference for warping, but we have shown that, for low values of the regressand (close the zero), the warped GP is much better, as it makes no impossible negative predictions. We have shown that combining warping and parse kernel approximations is feasible and the training is no more difficult than without the warping.\n",
    "\n",
    "In addition, we have provided a proof-of-concept demonstration that warping, input uncertainty and  sparse kernel approximations can be combined simply within Vanguard and trained successfully."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9ebed8d14db3dfb",
   "metadata": {},
   "source": [
    "Distributed GPs\n",
    "===============\n",
    "\n",
    "In this notebook we introduce Gaussian process distribution, allowing the training of Gaussian processes quickly on large-scale data. In the future, we hope to offer (cloud) support for using distributed GPs in an embarrassingly parallel way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1dbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gpytorch.utils.linear_cg\", lineno=234)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gpytorch.utils.linear_cg\", lineno=266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cf037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "\n",
    "from vanguard.datasets.bike import BikeDataset\n",
    "from vanguard.distribute import Distributed, aggregators, partitioners\n",
    "from vanguard.vanilla import GaussianGPController\n",
    "from vanguard.warps import SetWarp, warpfunctions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cbe9ff5b5f52a67",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "Recall that the complexity of training a Gaussian process and then making a prediction is :math:`\\mathcal{O}(n^3)` and :math:`\\mathcal{O}(n^2)` respectively when using exact inference. One way to overcome this is to use `variational inference <sparse_variational_gps.ipynb`>_, but this uses non-exact inference which may not be the best fit. An alternative is to distribute the computations across multiple sources of compute and run exact inference in parallel. This is done by *partitioning* the data into :math:`m` disjoint sets, and then running inference on all of them. The computational complexity of training under exact inference is therefore reduced to :math:`\\mathcal{O}\\left(\\frac{n^3}{m^2}\\right)`, and *further* reduced to :math:`\\mathcal{O}\\left(\\frac{n^3}{m^3}\\right)` when this occurs in parallel. Predictions can then be made across these disjoint models, and *aggregated* into a final prediction presented to the user."
   ]
  },
  {
   "cell_type": "raw",
   "id": "62190198",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "We will use 5,000 data points from the :py:class:`~vanguard.datasets.bike.BikeDataset`. The data has 13 features. The target variable :math:`y` is always non-negative, so warping could be useful. This dataset is taken from :cite:`FanaeeT2013` and was accessed and copied to Github LFS within this repo on 1st July 2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = BikeDataset(n_samples=5000, training_proportion=0.9, noise_scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "DATASET.plot_y()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82a6f0cc",
   "metadata": {},
   "source": [
    "Baseline\n",
    "--------\n",
    "\n",
    "We'll start with an Exact GP baseline, this is the most accurate method but can be too slow/memory intensive for large data sets. We will begin with a scaled :py:class:`~gpytorch.kernels.MaternKernel` with a standard :py:class:`~vanguard.vanilla.GaussianGPController`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledMaternKernel(ScaleKernel):\n",
    "    \"\"\"A scaled Matern kernel.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(MaternKernel(nu=1.5, ard_num_dims=DATASET.train_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianGPController(\n",
    "    train_x=DATASET.train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    likelihood_kwargs={\"learn_additional_noise\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d415caef87d1765d",
   "metadata": {},
   "source": [
    "We will use a timer to showcase how fast the training process is under each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer():\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end-start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef2d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer():\n",
    "    loss = gp.fit(n_sgd_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942382ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(DATASET.test_x)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "DATASET.plot_prediction(*posterior.confidence_interval())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "765ff75a",
   "metadata": {},
   "source": [
    "We can improve our baseline by adding compositional warping to the Gaussian GP controller. We'll use an affine-log warp to reflect the non-negativity of the data: :math:`\\phi(y) = a + b\\log(y)`.\n",
    "\n",
    "The code to create this model in Vanguard is simple, we just add the :py:class:`~vanguard.warps.SetWarp` decorator to our existing controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = warpfunctions.AffineWarpFunction(a=3, b=-1) @ warpfunctions.BoxCoxWarpFunction(0.2)\n",
    "\n",
    "\n",
    "@SetWarp(warp, ignore_all=True)\n",
    "class WarpedGPController(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf05ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = WarpedGPController(\n",
    "    train_x=DATASET.train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    likelihood_kwargs={\"learn_additional_noise\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc06894",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer():\n",
    "    loss = gp.fit(n_sgd_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(DATASET.test_x)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "DATASET.plot_prediction(*posterior.confidence_interval())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9cb4ec6",
   "metadata": {},
   "source": [
    "Distributed GPs\n",
    "---------------\n",
    "\n",
    "Distributed GPs can be created with the :py:class:`~vanguard.distribute.decorator.Distributed` decorator applied over a controller. The decorator has three main arguments:\n",
    "\n",
    "* ``n_experts`` - The number of experts (separate Gaussian processes) to use. Fewer experts make the model slower but tend to give more accurate predictions.\n",
    "* ``partitioner_class`` - This class controls how experts are assigned data points.\n",
    "* ``aggregator_class`` - This class controls how expert predictions are aggregated. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4456799a497ddf51",
   "metadata": {},
   "source": [
    "Partitioning\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "There are a number of ways to partition data, with varied results on the fit of the overall model. Since each pot of data will be used to train a Gaussian process, it makes sense to partition data into coherent sections to make this easier. The most basic partition strategy is to group the data randomly, which sets a baseline for the process. This is equivalent to training a Gaussian process on a down-sample of the original data, and so one would expect a decrease in quality of fit across all models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8436f028",
   "metadata": {},
   "source": [
    "A much better choice is the :py:class:`~vanguard.distribute.partitioners.KMeansPartitioner`, which will cluster the training data to divide it into more appropriate sections for each expert. This way, each model has a better chance of fitting, as each cluster will hopefully represent a cohesive area of the data which can be more easily modelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioner = partitioners.KMeansPartitioner(DATASET.train_x, n_experts=5)\n",
    "partition = partitioner.create_partition()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ee18b55aac6057f",
   "metadata": {},
   "source": [
    "We can use a T-SNE plot to view how the training data has been partitioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "partitioner.plot_partition(partition)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "068f3853",
   "metadata": {},
   "source": [
    ":py:class:`~vanguard.distribute.partitioners.KMeansPartitioner` is almost always best unless you are using a \"non-Euclidean\" kernel, in which case :py:class:`~vanguard.distribute.partitioners.KMedoidsPartitioner` can give better results. The latter uses :math:`\\mathcal{O}(n^2)` memory so can only be used for small data sets (:math:`\\sim10^4` data points)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "adbc4cbc",
   "metadata": {},
   "source": [
    "Aggregating\n",
    "~~~~~~~~~~~\n",
    "\n",
    "Once the training data has been partitioned, it is distributed to each of the expert controllers, who can then run their own inference in parallel to tune the hyperparameters. When it comes to making a prediction, each expert called and their results are pooled together into an overall posterior. Similarly to partitioning, there are a number of different methods available for doing this, but the most straightforward is the \"product of experts\" method from :cite`Deisenroth15`, implemented with the :py:class:`~vanguard.distribute.aggregators.POEAggregator`. Given the posteriors of the experts :math:`p_{i}(y|x) = N(\\mu_{i}(x), \\sigma_{i}^{2}(x))` for :math:`i=1, 2, ..., m`, we define the joint posterior as a Gaussian with moments:\n",
    "\n",
    ".. math::\n",
    "    \\mu &= \\sigma^{2} \\sum_{i} \\sigma_{i}^{-2}(x) \\mu_{i}(x) \\\\\n",
    "    \\sigma^{-2} &= \\sum_{i} \\sigma_{i}^{-2}(x).\n",
    "   \n",
    "However, the :py:class:`~vanguard.distribute.aggregators.XGRBCMAggregator` is theoretically the best choice (in the limit of ``n_experts``), but the :py:class:`~vanguard.distribute.aggregators.RBCMAggregator` works well in practice."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2702aefedb57962f",
   "metadata": {},
   "source": [
    "Distribution in Vanguard\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "In Vanguard, things work a little differently. Instead of training experts separately, hyperparameters are tuned on a random subset of the training data. The trained mean and kernel (and any other trained components from other decorators) are then passed to each expert for use in inference, which saves overall computation. This makes the fitting process very efficient."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31119f9",
   "metadata": {},
   "source": [
    ".. warning::\n",
    "\n",
    "    The :py:class:`~vanguard.distribute.decorator.Distributed` decorator currently only accepts numerical values \n",
    "    for the ``y_std`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXPERTS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Distributed(\n",
    "    n_experts=N_EXPERTS,\n",
    "    subset_fraction=1 / N_EXPERTS,\n",
    "    aggregator_class=aggregators.XGRBCMAggregator,\n",
    "    partitioner_class=partitioners.KMeansPartitioner,\n",
    "    ignore_methods=(\"__init__\",),\n",
    ")\n",
    "class DistributedGPController(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = DistributedGPController(\n",
    "    train_x=DATASET.train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std[0],\n",
    "    likelihood_kwargs={\"learn_additional_noise\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704de87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer():\n",
    "    loss = gp.fit(n_sgd_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a21199",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(DATASET.test_x)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "DATASET.plot_prediction(*posterior.confidence_interval())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "751f1f5080077cdd",
   "metadata": {},
   "source": [
    "Again we can add compositional warping to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63494b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = warpfunctions.AffineWarpFunction(a=3, b=-1) @ warpfunctions.BoxCoxWarpFunction(0.2)\n",
    "\n",
    "\n",
    "@Distributed(\n",
    "    n_experts=N_EXPERTS,\n",
    "    subset_fraction=1 / N_EXPERTS,\n",
    "    aggregator_class=aggregators.XGRBCMAggregator,\n",
    "    partitioner_class=partitioners.KMeansPartitioner,\n",
    "    ignore_all=True,\n",
    ")\n",
    "@SetWarp(warp, ignore_all=True)\n",
    "class WarpedDistributedGPController(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f715ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = WarpedDistributedGPController(\n",
    "    train_x=DATASET.train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=ScaledMaternKernel,\n",
    "    y_std=DATASET.train_y_std[0],\n",
    "    likelihood_kwargs={\"learn_additional_noise\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ae042",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer():\n",
    "    loss = gp.fit(n_sgd_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gp.posterior_over_point(DATASET.test_x)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "DATASET.plot_prediction(*posterior.confidence_interval())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44189ce52a999c49",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "-----------\n",
    "\n",
    "We have demonstrated that while distributed GPs are not as accurate as the exact GP, they are much more scalable. For large data sets, where runtime and memory are key considerations, the distributed GP may be the preferred model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

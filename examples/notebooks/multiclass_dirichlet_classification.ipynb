{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3e2aee20ac1e6654",
   "metadata": {},
   "source": [
    "Multiclass Classification with Dirichlet Distributions\n",
    "======================================================\n",
    "\n",
    "An alternative implementation for  multiclass classification in Vanguard. This methodology is based on `this example notebook <https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_on_Classification_Labels.html>`_.\n",
    "To get started with multi-class classification, make sure you check out the `multi-class classification example <multiclass_classification.ipynb>`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "random_seed = 1_989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ZeroMean\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vanguard.classification import DirichletMulticlassClassification\n",
    "from vanguard.classification.kernel import DirichletKernelMulticlassClassification\n",
    "from vanguard.classification.likelihoods import (\n",
    "    DirichletKernelClassifierLikelihood,\n",
    "    GenericExactMarginalLogLikelihood,\n",
    ")\n",
    "from vanguard.datasets.classification import MulticlassGaussianClassificationDataset\n",
    "from vanguard.kernels import ScaledRBFKernel\n",
    "from vanguard.vanilla import GaussianGPController"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d30437d",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "Recall that in `standard multi-class classification <multiclass_classification.ipynb>`_, one is essentially training a binary classifier for each distinct class, taking advantage of Vanguard components to ensure that covariance between them is properly ascertained. In this example, we consider a different method for multi-class classification, where we regress directly onto the target label probability distributions.  The theory behind this is explored fully in sections 4 and 4.1 of :cite:`Milios18`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4951600afcc8592b",
   "metadata": {},
   "source": [
    "Consider a classification problem over $m$ classes. The aim is to infer a posterior distribution for each prediction, from which we draw a multinomial over the classes :math:`\\pi = (\\pi_1,\\dots,\\pi_m)`, corresponding to the class probabilities. This distribution takes the form of a `Dirichlet model <https://en.wikipedia.org/wiki/Dirichlet_distribution>`_ parameterised by some :math:`m`-dimensional vector :math:`\\alpha`:\n",
    "\n",
    ".. math:: \n",
    "    \\pi \\sim \\text{Dir}(\\alpha), \\quad \\alpha=(\\alpha_1,\\dots,\\alpha_m).\n",
    "\n",
    "A `common way <https://en.wikipedia.org/wiki/Dirichlet_distribution#Related_distributions>`_ to generate a sample from a Dirichlet distribution is to instead consider :math:`m` independent random `Gamma distributions <https://en.wikipedia.org/wiki/Gamma_distribution>`_. If we have that\n",
    "\n",
    ".. math::\n",
    "    x_i \\sim \\text{Gamma}(\\alpha_i,1)\n",
    "\n",
    "and we define\n",
    "\n",
    ".. math::\n",
    "    \\pi_i = \\frac{x_i}{\\sum_{j=1}^mx_j}\n",
    "\n",
    "then\n",
    "\n",
    ".. math:: \n",
    "    \\pi_1,\\cdots,\\pi_m \\sim \\text{Dir}(\\alpha)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "511938f58a6ef471",
   "metadata": {},
   "source": [
    "Recall that the output from a Gaussian process is a normal distribution, and it is difficult to \"transition\" samples from such a distribution to a Gamma distribution in order to generate our Dirichlet sample. Instead of using a Gamma directly, we instead approximate it with a `Log-normal distribution <https://en.wikipedia.org/wiki/Log-normal_distribution>`_. Consider our approximate random variable :math:`\\bar{x}_i`:\n",
    "\n",
    ".. math:: \n",
    "    \\bar{x}_i \\sim \\text{Lognormal}(\\bar{\\mu}_i,\\bar{\\sigma}_i^2).\n",
    "\n",
    "The mean and variance of :math:`\\bar{x}_i` are given by:\n",
    "\n",
    ".. math::\n",
    "    \\begin{align}\n",
    "        \\text{E}[\\bar{x}_i] &= \\exp\\left(\\bar{\\mu}_i + \\frac{\\bar{\\sigma}_i^2}{2}\\right) \\\\\n",
    "        \\text{Var}[\\bar{x}_i] &= \\left(\\exp(\\bar{\\sigma}_i^2) - 1\\right) \\exp\\left(2\\bar{\\mu}_i + \\bar{\\sigma}_i^2\\right).\n",
    "    \\end{align}\n",
    "\n",
    "Given that :math:`\\text{E}[x_i] = \\text{Var}[x_i] = \\alpha_i`, it is possible to deduce the values for :math:`\\bar{\\mu}_i` and :math:`\\bar{\\sigma}_i^2` to match these:\n",
    "\n",
    ".. math::\n",
    "    \\bar{\\sigma}_i^2 = \\log\\left(\\frac{1}{\\alpha_i} + 1\\right), \\quad \\bar{\\mu}_i = \\log\\left(\\alpha_i - \\frac{\\bar{\\sigma}_i^2}{2} \\right).\n",
    "\n",
    "This can be verified with the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_i = 0.86858729\n",
    "sigma_squared_i = np.log(1 / alpha_i + 1)\n",
    "mu_i = np.log(alpha_i - sigma_squared_i / 2)\n",
    "\n",
    "n_samples = 10_000\n",
    "\n",
    "random_generator = np.random.Generator(np.random.PCG64(seed=random_seed))\n",
    "gamma_samples = random_generator.gamma(shape=alpha_i, scale=1.0, size=n_samples)\n",
    "lognormal_samples = random_generator.lognormal(mean=mu_i, sigma=np.sqrt(sigma_squared_i), size=n_samples)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "n_bins = 150\n",
    "plt.hist(gamma_samples, bins=n_bins, density=True, alpha=0.6, label=\"gamma\")\n",
    "plt.hist(lognormal_samples, bins=n_bins, density=True, alpha=0.6, label=\"lognormal\")\n",
    "plt.xlim(right=8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8ee7544",
   "metadata": {},
   "source": [
    "If a random variable :math:`X` is normally-distributed, then :math:`\\exp(X)` is log-normally distributed. Given that the output of the Gaussian process model is a Gaussian distribution, then this means we can follow the above steps in reverse to sample from :math:`\\text{Dir}(\\alpha)`. By maximising the likelihood, we infer the parameters to regress onto the correct probability distributions :math:`\\pi`.  This is all taken care of within the :py:class:`~gpytorch.likelihoods.DirichletClassificationLikelihood`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebc9be3f",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "We start with the :py:class:`~vanguard.datasets.classification.MulticlassGaussianClassificationDataset` for this experiment, which creates multiple classes based on the distance to the centre of a two-dimensional Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "\n",
    "DATASET = MulticlassGaussianClassificationDataset(\n",
    "    num_train_points=100,\n",
    "    num_test_points=500,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    covariance_scale=1,\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a207b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d08f666f",
   "metadata": {},
   "source": [
    "Instead of the :py:class:`~vanguard.classification.categorical.CategoricalClassification` decorator we used before, we instead use the :py:class:`~vanguard.classification.dirichlet.DirichletMulticlassClassification` decorator. Note that we no longer require the :py:class:`~vanguard.variational.VariationalInference` decorator - this works with exact inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DirichletMulticlassClassification(num_classes=NUM_CLASSES, ignore_methods=(\"__init__\",))\n",
    "class MulticlassGaussianClassifier(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "326214ef",
   "metadata": {},
   "source": [
    "We require the :py:class:`~gpytorch.likelihoods.DirichletClassificationLikelihood`, and also need to ensure that the correct batch shape is passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = MulticlassGaussianClassifier(\n",
    "    DATASET.train_x,\n",
    "    DATASET.train_y,\n",
    "    ScaledRBFKernel,\n",
    "    y_std=0,\n",
    "    mean_class=ZeroMean,\n",
    "    likelihood_class=DirichletClassificationLikelihood,\n",
    "    mean_kwargs={\"batch_shape\": (NUM_CLASSES,)},\n",
    "    kernel_kwargs={\"batch_shape\": (NUM_CLASSES,)},\n",
    "    likelihood_kwargs={\"alpha_epsilon\": 0.3, \"learn_additional_noise\": True},\n",
    "    optim_kwargs={\"lr\": 0.05},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.fit(100)\n",
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a3012739f7e27c4",
   "metadata": {},
   "source": [
    "Note that the model does surprisingly well without fitting, but that fitting does not seem to make much of a difference."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7059b110",
   "metadata": {},
   "source": [
    "Dirichlet Kernel Approximation Modelling\n",
    "----------------------------------------\n",
    "\n",
    "An alternative method for modelling uses the :py:class:`~vanguard.classification.kernel.DirichletKernelMulticlassClassification` decorator. It requires the :py:class:`~vanguard.classification.likelihoods.DirichletKernelClassifierLikelihood` and the :py:class:`~vanguard.classification.likelihoods.GenericExactMarginalLogLikelihood` special marginal log likelihood.\n",
    "\n",
    "This method is based on :cite:`MacKenzie14`. It is is a kernel machine method with a Dirichlet likelihood. The posterior over the classes is\n",
    "\n",
    ".. math::\n",
    "\n",
    "    \\text{Dir}\\left(\\alpha_1 + \\frac{N_{-1}}{N}\\sum_{j\\mid y_j=1} k(x_i, x_j), \\ldots, \\alpha_m + \\frac{N_{-D}}{N}\\sum_{j\\mid y_j=N}k(x_i, x_j)\\right)\n",
    "    \n",
    "where :math:`N` is the total number of data points, :math:`N_{-i}` is the total number of data points that are not in class :math:`i` and :math:`m` is the total number of classes. :math:`y_j` is the class of the :math:`j`-th training data point. Here :math:`\\alpha` is a hyperparameter (it is the usual Dirichlet prior hyperparameter) and can be tuned like any other if desired. The kernel :math:`k` is just like any other kernel in GP modelling. \n",
    "\n",
    ".. note::\n",
    "\n",
    "    This model is not actually a GP, but is similar enough in practice to warrant its inclusion in Vanguard (it is a non-parametric kernel classifier with a Dirichlet likelihood). In addition, there may exist a formulation in which this method is an approximation to a GP posterior, but we have not been able to find this formulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DirichletKernelMulticlassClassification(num_classes=NUM_CLASSES, ignore_methods=(\"__init__\",))\n",
    "class MulticlassGaussianClassifier(GaussianGPController):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = MulticlassGaussianClassifier(\n",
    "    DATASET.train_x,\n",
    "    DATASET.train_y,\n",
    "    kernel_class=ScaledRBFKernel,\n",
    "    y_std=0,\n",
    "    mean_class=ZeroMean,\n",
    "    likelihood_class=DirichletKernelClassifierLikelihood,\n",
    "    likelihood_kwargs={\"learn_alpha\": False, \"alpha\": 5},\n",
    "    marginal_log_likelihood_class=GenericExactMarginalLogLikelihood,\n",
    "    optim_kwargs={\"lr\": 0.1, \"early_stop_patience\": 5},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf57332",
   "metadata": {},
   "outputs": [],
   "source": [
    "with controller.metrics_tracker.print_metrics(every=25):\n",
    "    controller.fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7939fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2a52f65a6602060",
   "metadata": {},
   "source": [
    "This model seems prone to overfitting in the kernel hyperparameters but particularly so in :math:`\\alpha` as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba278ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = MulticlassGaussianClassifier(\n",
    "    DATASET.train_x,\n",
    "    DATASET.train_y,\n",
    "    kernel_class=ScaledRBFKernel,\n",
    "    y_std=0,\n",
    "    mean_class=ZeroMean,\n",
    "    likelihood_class=DirichletKernelClassifierLikelihood,\n",
    "    likelihood_kwargs={\"learn_alpha\": True, \"alpha\": 5},\n",
    "    marginal_log_likelihood_class=GenericExactMarginalLogLikelihood,\n",
    "    optim_kwargs={\"lr\": 0.1, \"early_stop_patience\": 5},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with controller.metrics_tracker.print_metrics(every=25):\n",
    "    controller.fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d550e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probs = controller.classify_points(DATASET.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_prediction(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "DATASET.plot_confusion_matrix(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31a328e4d3b94bed",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "-----------\n",
    "\n",
    "It is unlikely that Dirichlet multi-class classification is ever worth using over previous techniques, but it is interesting how powerful it is without any training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

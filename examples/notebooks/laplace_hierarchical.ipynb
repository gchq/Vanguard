{
 "cells": [
  {
   "cell_type": "raw",
   "id": "38ca1657",
   "metadata": {},
   "source": [
    "Bayesian treatment of hyperparameters with Laplace approximations\n",
    "================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb053b92264bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â© Crown Copyright GCHQ\n",
    "#\n",
    "# Licensed under the GNU General Public License, version 3 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.gnu.org/licenses/gpl-3.0.en.html\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faacedabde0a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is not compiled into the documentation due to the time taken to run it to get\n",
    "# a representative analysis. Please run this notebook locally if you wish to see the outputs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4db687b17cdf61b1",
   "metadata": {},
   "source": [
    "Gaussian process models are already Bayesian, in that an unknown function is given a GP prior and then a posterior is inferred over that function. Typically, however, the GP prior is defined using hyperparameters that are contained in the prior mean and kernel functions. There can also be hyperparameters in the likelihood. Sometimes one might have a good idea of what these prior hyperparameters should be (e.g. if the kernel is periodic and the data have some known periodicity). Otherwise, it is standard to learn good hyperparameters by optimising the log marginal likelihood with respect to them. This process yields point estimates of the hyperparameters and is not Bayesian. In a lot of practical applications, there will be considerable prior uncertainty about the value of these hyperparameters, and point estimates obtained from likelihood maximisation will yield over-confident posteriors. The correct thing to do is place a prior over the hyperparameters themselves and then infer the posterior. The full posterior process is then \n",
    "\n",
    ".. math::\n",
    "\n",
    "    p(f \\mid \\mathcal{D}) = \\int d\\theta p(\\theta | \\mathcal{D}) p(f \\mid \\mathcal{D}, \\theta)\n",
    "    \n",
    "Here :math:`p(f \\mid \\mathcal{D}, \\theta)` is the usual GP posterior with some fixed hyperparameters :math:`\\theta` and :math:`p(\\theta\\mid \\mathcal{D})` is the posterior distribution over the hyperparameters. Apart from the extra uncertainty accounted for by this approach, note also that, even if the conditional GP posteriors  :math:`p(f \\mid \\mathcal{D}, \\theta)`  are Gaussian, the overall posterior above will not be, so the Bayesian treatment of hyperparameters allows for a much richer class of posteriors.\n",
    "\n",
    "We showcase how to approximate the intractable hyperparameter posterior :math:`p(\\theta \\mid \\mathcal{D})` using an approach based on the Laplace approximation.\n",
    "\n",
    "Suppose that log marginal likelihood maximisation has produced optimised hyperparameters :math:`\\theta_*`. The Laplace approximation to :math:`p(\\theta \\mid \\mathcal{D})` is then \n",
    "\n",
    ".. math::\n",
    "\n",
    "    p(\\theta \\mid \\mathcal{D}) \\approx \\mathcal{N}(\\theta \\mid \\theta_*, H^{-1})\n",
    "    \n",
    "where the matrix :math:`H` is the Hessian of log marginal likelihood at :math:`\\theta_*`, i.e. \n",
    "\n",
    ".. math::\n",
    "\n",
    "    H = \\frac{\\partial^2 L}{\\partial \\theta^2}\\Bigg|_{\\theta=\\theta_*}\n",
    "    \n",
    "where :math:`L(\\theta) = \\log p(\\mathcal{D}\\mid \\theta)` (under the GP model).\n",
    "\n",
    "The Laplace approximation has been found to be a competitive approach in, say, Bayesian neural networks. It has the great advantage of being simple, and particularly in the case of GP hyperparameters, it is very efficient compared to other approaches, since the dimension of :math:`\\theta` is typically small, so exact Hessians can be computed using automatic differentiation.\n",
    "\n",
    "A clear issue with this approach as described is the need to invert :math:`H`. In practice one finds that the log marginal likelihood surfaces contains at least some very flat directions around :math:`\\theta_*`, i.e. :math:`H` has some very small eigenvalues. These correspond to directions in which the Laplace approximation fails (or nearly fails), since it is only valid in the case that :math:`H` is positive-definite. Note that the Hessian will not generically contain any negative eigenvalues, except very small ones in the almost-flat directions; this is to be expected if we assume that the optimisation procedure can escape such obvious saddle points and only becomes stuck in approximate local minima. Naively inverting the Hessian in the presence of such small eigenvalues will, at best, result in a covariance matrix with some extremely large variances, and at worst lead to an invalid indefinite matrix. The latter problem can be mitigated with covariance cleaning techniques such as linear shrinkage, i.e. replace :math:`H` by :math:`(1-\\beta)H + \\beta I` for some small :math:`\\beta\\in(0, 1)`, however this will not solve the former problem.\n",
    "\n",
    "Our aim here is not to provide perfect representation of the hyperparameter posteriors, rather we aim simply to provide *some* improvement in uncertainty quantification over the baseline approach of plain marginal likelihood maximisation with point estimates. We therefore accept that the full Laplace approximation is not available and restrict only to directions in which the Hessian eigenvalues are not too small. More precisely, use an eigendecomposition :math:`H = U^T\\Lambda U` and let :math:`r(\\Lambda)` be a diagonal matrix with :math:`r(\\Lambda)_i = (\\lambda_i)^{-1}` if :math:`\\lambda_i > \\epsilon` and :math:`r(\\Lambda)_i = \\eta` otherwise, where :math:`\\epsilon,\\eta>0` are small parameters. We then replace :math:`H^{-1}` in the Laplace approximation by :math:`\\Sigma = U^T r(\\Lambda)U`.\n",
    "\n",
    "Thus :math:`\\Sigma` preserves the covariance structure of the Laplace approximation in the well-behaved directions, while essentially using point estimates in the badly-behaved directions. Note however that the eigendirections of :math:`H` are not the same as the coordinate directions corresponding to the individual hyperparameters themselves, so our approach is not the same as treating certain hyperparameters as point estimates (though in practice we expect that this may be approximately the case).\n",
    "\n",
    "Even with the above regularisation of the Laplace approximation covariance matrix, the resulting posterior process :math:`\\int d\\theta p(\\cdot \\mid \\theta, \\mathcal{D})p(\\theta \\mid \\mathcal{D})` may still have impractically large posterior predictive uncertainty. Without a definitive way of saying that this accurate, we must be pragmatic and seek to make the posterior predictive actually useful. In the Bayesian deep learning literature, it has been found that cold posteriors can give superior approximations to Bayesian NN posteriors. In our case this amounts to \n",
    "\n",
    ".. math::\n",
    "\n",
    "    p(\\theta \\mid \\mathcal{D}) \\approx \\mathcal{N}(\\theta \\mid \\theta_*, T\\Sigma)\n",
    " \n",
    "where :math:`T>0` is a temperature parameter, and :math:`T<1` corresponds to the cold posterior regime. \n",
    "\n",
    "All this in place, we have a practical method for providing users with some means of accounting for the uncertainty in their models' hyperparameters. At :math:`T=0` we recover exactly the point estimate hyperparameter posterior, so by gradually increasing :math:`T` the user can explore, in a principled and efficient way, the posterior uncertainty that is hidden by their point estimate hyperparameter estimation. The temperature also provides another parameter to tune to maximise marginal log probability and so hopefully provide some greater robustness to overfitting.\n",
    "\n",
    "\n",
    ".. toctree::\n",
    "   :maxdepth: 1\n",
    "   :hidden:\n",
    "\n",
    ".. _hierarchical example:\n",
    "  ./hierarchical.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2243925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx ignore\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5ac62a5755e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1_989\n",
    "num_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gpytorch import constraints, kernels, likelihoods, means\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vanguard.datasets.air_passengers import AirPassengers\n",
    "from vanguard.datasets.synthetic import SyntheticDataset, complicated_f\n",
    "from vanguard.hierarchical import BayesianHyperparameters, LaplaceHierarchicalHyperparameters\n",
    "from vanguard.learning import LearnYNoise\n",
    "from vanguard.normalise import NormaliseY\n",
    "from vanguard.vanilla import GaussianGPController"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d476ed22",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "We will use :py:class:`~vanguard.datasets.synthetic.SyntheticDataset` to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52eb401-6bca-4fc0-baf3-b5592d26e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = SyntheticDataset(functions=(complicated_f,), rng=np.random.default_rng(random_seed))\n",
    "train_test_split_index = len(DATASET.train_x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7b0cd21",
   "metadata": {},
   "source": [
    "Modelling\n",
    "---------\n",
    "\n",
    "Let's start by constructing a standard GP models with point estimate hyperparameters for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e941624-1058-4a02-ab42-9332dba83925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledRBFKernel(kernels.ScaleKernel):\n",
    "    def __init__(self, active_dims=None, batch_shape=torch.Size([])):\n",
    "        super().__init__(\n",
    "            kernels.RBFKernel(active_dims=active_dims, batch_shape=batch_shape),\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "\n",
    "\n",
    "class ScaledMaternKernel(kernels.ScaleKernel):\n",
    "    def __init__(self, active_dims=None, batch_shape=torch.Size([])):\n",
    "        super().__init__(\n",
    "            kernels.MaternKernel(nu=0.5, active_dims=active_dims, batch_shape=batch_shape),\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "\n",
    "\n",
    "class ScaledPeriodicKernel(kernels.ScaleKernel):\n",
    "    def __init__(self, active_dims=None, batch_shape=torch.Size([])):\n",
    "        super().__init__(\n",
    "            kernels.PeriodicKernel(active_dims=active_dims, batch_shape=batch_shape),\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "\n",
    "\n",
    "class Kernel(kernels.ProductKernel):\n",
    "    def __init__(self, batch_shape=torch.Size([])):\n",
    "        super().__init__(\n",
    "            ScaledRBFKernel(batch_shape=batch_shape),\n",
    "            kernels.PeriodicKernel(batch_shape=batch_shape),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b84c0-e1a5-4932-9a8b-9b7d39b04270",
   "metadata": {},
   "outputs": [],
   "source": [
    "@LearnYNoise(ignore_all=True)\n",
    "class PointEstimateController(GaussianGPController):\n",
    "    pass\n",
    "\n",
    "\n",
    "gp = PointEstimateController(\n",
    "    train_x=DATASET.train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=Kernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    optim_kwargs={\"lr\": 0.5},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "\n",
    "with gp.metrics_tracker.print_metrics(every=20):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gp.fit(n_sgd_iters=num_iters)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    posterior = gp.posterior_over_point(DATASET.test_x)\n",
    "    likelihood = gp.predictive_likelihood(DATASET.test_x)\n",
    "\n",
    "mu, lower, upper = posterior.confidence_interval()\n",
    "l_mu, l_lower, l_upper = likelihood.confidence_interval()\n",
    "\n",
    "plt_x = DATASET.test_x.ravel()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(plt_x, l_mu, label=\"likelihood\")\n",
    "plt.fill_between(plt_x, l_lower, l_upper, alpha=0.2, label=\"likelihood CI\")\n",
    "plt.plot(plt_x, DATASET.test_y, \"x\", label=\"data\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend()\n",
    "print(f\"Log probability: {likelihood.log_probability(DATASET.test_y)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bfae3c9",
   "metadata": {},
   "source": [
    "Now we will convert this model to use Bayesian inference over its hyperparameters. \n",
    "\n",
    "Any kernels or means that are to be given Bayesian hyperparameters must be decorated with :py:class:`~vanguard.hierarchical.module.BayesianHyperparameters`. This may seem clunky, but it allows for fine-grained control over which hyperparameters are made Bayesian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@BayesianHyperparameters()\n",
    "class BayesianRBFKernel(kernels.RBFKernel):\n",
    "    pass\n",
    "\n",
    "\n",
    "@BayesianHyperparameters()\n",
    "class BayesianPeriodicKernel(kernels.PeriodicKernel):\n",
    "    pass\n",
    "\n",
    "\n",
    "@BayesianHyperparameters()\n",
    "class BayesianScaleKernel(kernels.ScaleKernel):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BayesianScaledRBFKernel(BayesianScaleKernel):\n",
    "    def __init__(self, active_dims=None, batch_shape=torch.Size([])):\n",
    "        super().__init__(\n",
    "            BayesianRBFKernel(active_dims=active_dims, batch_shape=batch_shape),\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "\n",
    "\n",
    "class BayesianScaledPeriodicKernel(BayesianScaleKernel):\n",
    "    def __init__(self, batch_shape=torch.Size([]), active_dims=None):\n",
    "        super().__init__(\n",
    "            BayesianPeriodicKernel(active_dims=active_dims, batch_shape=batch_shape),\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "\n",
    "\n",
    "class BayesianKernel(kernels.ProductKernel):\n",
    "    def __init__(self, batch_shape=torch.Size([])):\n",
    "        super().__init__(\n",
    "            BayesianScaledRBFKernel(batch_shape=batch_shape),\n",
    "            BayesianPeriodicKernel(batch_shape=batch_shape),\n",
    "        )\n",
    "\n",
    "\n",
    "@BayesianHyperparameters()\n",
    "class BayesianConstantMean(means.ConstantMean):\n",
    "    pass\n",
    "\n",
    "\n",
    "@BayesianHyperparameters()\n",
    "class BayesianFixedNoiseGaussianLikelihood(likelihoods.FixedNoiseGaussianLikelihood):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54c94596-4214-4924-a17a-e1b6f833a418",
   "metadata": {},
   "source": [
    "The decorator :py:class:`~vanguard.hierarchical.laplace.LaplaceHierarchicalHyperparameters` converts a controller to approximate the hyperparameter posterior use a Laplace approximation. The argument ``num_mc_samples`` defines the number of samples to draw from the variational hyperparameter posterior distribution when approximating integrals using Monte Carlo integration.\n",
    "\n",
    "We can specify a temperature for the hyperparameter posterior in the :py:class:`~vanguard.hierarchical.laplace.LaplaceHierarchicalHyperparameters` decorator, but leaving it blank will set the temperature automatically using a heuristic (to give a covariance matrix with unit trace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d51a02-fbde-41b8-9afc-1e7ecc4ede14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@LaplaceHierarchicalHyperparameters(num_mc_samples=100, ignore_all=True)\n",
    "class FullBayesianController(PointEstimateController):\n",
    "    pass\n",
    "\n",
    "\n",
    "gp = FullBayesianController(\n",
    "    train_x=DATASET.train_x,\n",
    "    train_y=DATASET.train_y,\n",
    "    kernel_class=BayesianKernel,\n",
    "    y_std=DATASET.train_y_std,\n",
    "    mean_class=BayesianConstantMean,\n",
    "    likelihood_class=BayesianFixedNoiseGaussianLikelihood,\n",
    "    optim_kwargs={\"lr\": 0.5},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "\n",
    "with gp.metrics_tracker.print_metrics(every=20):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gp.fit(n_sgd_iters=num_iters)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4e47010f2fc89fc",
   "metadata": {},
   "source": [
    "Let's have a look at the hyperparameter posterior mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1bac52-2532-48ba-8dfa-ee512cb28a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gp.hyperparameter_posterior.covariance_matrix.detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "print(gp.hyperparameter_posterior.mean.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666e03f-fe05-41dc-a97c-686890b294b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    posterior = gp.posterior_over_point(DATASET.test_x)\n",
    "    likelihood = gp.predictive_likelihood(DATASET.test_x)\n",
    "\n",
    "mu, lower, upper = posterior.confidence_interval()\n",
    "l_mu, l_lower, l_upper = likelihood.confidence_interval()\n",
    "plt_x = DATASET.test_x.ravel()\n",
    "\n",
    "# Convert from tensors to numpy arrays for plotting\n",
    "plt_x = plt_x.detach().cpu().numpy()\n",
    "l_mu = l_mu.detach().cpu().numpy()\n",
    "l_lower = l_lower.detach().cpu().numpy()\n",
    "l_upper = l_upper.detach().cpu().numpy()\n",
    "plt_y = DATASET.test_y.detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(plt_x, l_mu, label=\"likelihood\")\n",
    "plt.fill_between(plt_x, l_lower, l_upper, alpha=0.2, label=\"likelihood CI\")\n",
    "plt.plot(plt_x, plt_y, \"x\", label=\"data\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend()\n",
    "print(f\"Log probability: {likelihood.log_probability(DATASET.test_y)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4172298ad73364e",
   "metadata": {},
   "source": [
    "Let's look at some posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497c659-07e4-4ee1-aee8-21366fc115e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(posterior.sample(500).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5a48bd0e674bab",
   "metadata": {},
   "source": [
    "Let's try varying the temperature to see what gets the best posterior log probability on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef559211-532e-4c6f-a026-5782ed45c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.logspace(-5, 0, 20)\n",
    "log_probs = []\n",
    "for _ in tqdm(range(20)):\n",
    "    lp = []\n",
    "    for temperature in temps:\n",
    "        gp.temperature = temperature\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            likelihood = gp.predictive_likelihood(DATASET.test_x)\n",
    "        lp.append(likelihood.log_probability(DATASET.test_y))\n",
    "    log_probs.append(lp)\n",
    "\n",
    "log_probs = np.array(log_probs)\n",
    "plt.plot(temps, log_probs.T)\n",
    "plt.xscale(\"log\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15af788-25c6-461d-a1ea-078b4c1d30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_probs = np.mean(log_probs, axis=0)\n",
    "plt.plot(temps, mean_log_probs, label=\"empirical mean\")\n",
    "plt.vlines(\n",
    "    [gp.auto_temperature()],\n",
    "    [min(mean_log_probs)],\n",
    "    [max(mean_log_probs)],\n",
    "    linestyles=\"--\",\n",
    "    color=\"r\",\n",
    "    label=\"auto temperature\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"log probability\")\n",
    "plt.xlabel(\"temperature\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e8dc356a79583b6",
   "metadata": {},
   "source": [
    "It appears that the automatically selected temperature is pretty good."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d4951d8-6577-4e79-84f9-0b3f877b7ce2",
   "metadata": {},
   "source": [
    "Real data: airline delays\n",
    "-------------------------\n",
    "\n",
    "This dataset is taken from the Kats Repository in the Facebook research repo, see :cite:`Jiang_KATS_2022`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115428d-819e-4034-bd60-08d95f14b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AirPassengers()\n",
    "df = data._load_data()\n",
    "\n",
    "train_test_split_index = 100\n",
    "x = df.index.values.astype(float)\n",
    "y = df.y.values.astype(float)\n",
    "train_x, train_y = x[:train_test_split_index], y[:train_test_split_index]\n",
    "test_x, test_y = x[train_test_split_index:], y[train_test_split_index:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a2b51d855176de4",
   "metadata": {},
   "source": [
    "We'll build a kernel suitable for time series. We'll constrain the linear kernel as big values for its variance can easily lead to posterior blow-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb5dfe-6d11-45c6-b301-f063ada6e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_co_constraint = constraints.Interval(0.0, 1.0)\n",
    "\n",
    "\n",
    "class AirlineKernel(kernels.AdditiveKernel):\n",
    "    def __init__(self, batch_shape=torch.Size([])):\n",
    "        local_period = ScaledRBFKernel(batch_shape=batch_shape)\n",
    "        local_period *= kernels.PeriodicKernel(batch_shape=batch_shape)\n",
    "        linear = kernels.LinearKernel(\n",
    "            batch_shape=batch_shape,\n",
    "            variance_constraint=linear_co_constraint,\n",
    "        )\n",
    "        rbf = ScaledRBFKernel(batch_shape=batch_shape)\n",
    "        super().__init__(local_period, linear, rbf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "717f1bb475302724",
   "metadata": {},
   "source": [
    "We'll apply SoftPlus warping to impose positivity and some fixed affine rescaling to prevent numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e872e02-4338-4779-a79d-de755d7e0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@NormaliseY()\n",
    "@LearnYNoise(ignore_all=True)\n",
    "class PointEstimateController(GaussianGPController):\n",
    "    pass\n",
    "\n",
    "\n",
    "gp = PointEstimateController(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    kernel_class=AirlineKernel,\n",
    "    y_std=0,\n",
    "    optim_kwargs={\"lr\": 0.1},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "\n",
    "with gp.metrics_tracker.print_metrics(every=20):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gp.fit(n_sgd_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161485da-4b24-41a4-bac2-2ad48459120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    posterior = gp.posterior_over_point(x)\n",
    "    likelihood = gp.predictive_likelihood(x)\n",
    "\n",
    "mu, lower, upper = posterior.confidence_interval()\n",
    "l_mu, l_lower, l_upper = likelihood.confidence_interval()\n",
    "\n",
    "plt_x = x.ravel()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(plt_x, l_mu, label=\"likelihood\")\n",
    "plt.fill_between(plt_x, l_lower, l_upper, alpha=0.2, label=\"likelihood CI\")\n",
    "plt.plot(train_x, train_y, \"x\", label=\"train data\")\n",
    "plt.plot(test_x, test_y, \"o\", label=\"test data\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend()\n",
    "print(f\"Log probability: {likelihood.log_probability(torch.tensor(y))}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "820b5d0717bd1549",
   "metadata": {},
   "source": [
    "Below we just directly convert to kernel into a Bayesian one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39252951-d577-40b4-a02b-df209496cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@BayesianHyperparameters()\n",
    "class BayesianLinearKernel(kernels.LinearKernel):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BayesianAirlineKernel(kernels.AdditiveKernel):\n",
    "    def __init__(self, batch_shape=torch.Size([])):\n",
    "        periodic = BayesianPeriodicKernel(batch_shape=batch_shape)\n",
    "        local_period = BayesianScaledRBFKernel(batch_shape=batch_shape) * periodic\n",
    "        linear = BayesianLinearKernel(\n",
    "            batch_shape=batch_shape,\n",
    "            variance_constraint=linear_co_constraint,\n",
    "        )\n",
    "        rbf = BayesianScaledRBFKernel(batch_shape=batch_shape)\n",
    "        super().__init__(local_period, linear, rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eec4f-d9da-4b62-8518-6137b4eeae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@LaplaceHierarchicalHyperparameters(num_mc_samples=100, ignore_all=True)\n",
    "class FullBayesianController(PointEstimateController):\n",
    "    pass\n",
    "\n",
    "\n",
    "laplace_gp = FullBayesianController(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    kernel_class=BayesianAirlineKernel,\n",
    "    y_std=0,\n",
    "    mean_class=BayesianConstantMean,\n",
    "    likelihood_class=BayesianFixedNoiseGaussianLikelihood,\n",
    "    optim_kwargs={\"lr\": 0.1},\n",
    "    rng=np.random.default_rng(random_seed),\n",
    ")\n",
    "\n",
    "with laplace_gp.metrics_tracker.print_metrics(every=20):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        laplace_gp.fit(n_sgd_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110dd6a-de24-4b60-b6fa-3d6e2ce4c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    posterior = laplace_gp.posterior_over_point(x)\n",
    "    laplace_likelihood = laplace_gp.predictive_likelihood(x)\n",
    "\n",
    "laplace_mu, laplace_lower, laplace_upper = laplace_likelihood.confidence_interval()\n",
    "\n",
    "plt_x = x.ravel()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(plt_x, laplace_mu.detach().cpu().numpy(), label=\"likelihood\")\n",
    "plt.fill_between(\n",
    "    plt_x, laplace_lower.detach().cpu().numpy(), laplace_upper.detach().cpu().numpy(), alpha=0.2, label=\"likelihood CI\"\n",
    ")\n",
    "plt.plot(train_x, train_y, \"x\", label=\"train data\")\n",
    "plt.plot(test_x, test_y, \"o\", label=\"test data\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend()\n",
    "print(f\"Log probability: {laplace_likelihood.log_probability(torch.tensor(y))}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1f8cf2fbc7324",
   "metadata": {},
   "source": [
    "Let's have a look at the raw hyperparameter posterior covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e3700-199a-41d3-a5ef-25162e4e9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(laplace_gp.hyperparameter_posterior.covariance_matrix.detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "print(laplace_gp.hyperparameter_posterior.mean.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7161f5c-40ac-4be6-a1fb-0aa4204e1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.logspace(-5, 0, 20)\n",
    "log_probs = []\n",
    "for run_index in tqdm(range(20)):\n",
    "    lp = []\n",
    "    for temperature in temps:\n",
    "        try:\n",
    "            laplace_gp.temperature = temperature\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                likelihood = laplace_gp.predictive_likelihood(test_x)\n",
    "            lp.append(likelihood.log_probability(torch.tensor(test_y)))\n",
    "        except Exception:\n",
    "            print(f\"Skipping temperature {temperature} run {run_index+1} due to numerical issues\")\n",
    "            lp.append(np.nan)\n",
    "\n",
    "    log_probs.append(lp)\n",
    "\n",
    "log_probs = np.array(log_probs)\n",
    "plt.plot(temps, log_probs.T)\n",
    "plt.xscale(\"log\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91a36d-1ebf-41a5-9ad7-bd22b2af0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_probs = np.mean(log_probs, axis=0)\n",
    "plt.plot(temps, mean_log_probs, label=\"empirical mean\")\n",
    "plt.vlines(\n",
    "    [laplace_gp.auto_temperature()],\n",
    "    [min(mean_log_probs)],\n",
    "    [max(mean_log_probs)],\n",
    "    linestyles=\"--\",\n",
    "    color=\"r\",\n",
    "    label=\"auto temperature\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"log probability\")\n",
    "plt.xlabel(\"temperature\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
